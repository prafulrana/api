(function() {

  'use strict';
  var express = require('express');
  var router = express.Router();
  var mongojs = require('mongojs');
  var db = mongojs('mongodb://admin:admin123@ds063809.mongolab.com:63809/meantodo', ['todos']);

  /* GET home page. */
  router.get('/', function(req, res) {
    res.render('index');
  });

  router.get('/api/todos', function(req, res) {
    db.todos.find(function(err, data) {
      res.json(data);
    });
  });
  
  router.get('/longtext', function(req, res) {
    res.end("See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38] See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]See also[edit] Portal icon Free software portal Btrfs – for Linux Comparison of file systems ext4 HAMMER – a file system with a similar feature set for DragonFly BSD LFS – BSD Log Structured Filesystem List of file systems LVM – Logical Volume Manager (Linux), supports snapshots LZJB – data compression algorithm used in ZFS NILFS – a Linux file system with checksumming (but not scrubbing), also supporting snapshots ReFS – a Microsoft file system with built-in resiliency features Reiser4 Sun Open Storage Veritas File System and Veritas Volume Manager – similar to ZFS Versioning file systems – List of versioning file systems Write Anywhere File Layout – a similar file system by NetApp References[edit] ^ Jump up to: a b "What's new in Solaris 11 Express 2010.11" (PDF). Oracle. Retrieved November 17, 2010. Jump up ^ "1.1 What about the licensing issue?". Retrieved November 18, 2010. Jump up ^ "Status Information for Serial Number 85901629 (ZFS)". United States Patent and Trademark Office. Retrieved October 21, 2013. Jump up ^ Sean Michael Kerner (2013-09-18). "LinuxCon: OpenZFS moves Open Source Storage Forward". infostor.com. Retrieved 2013-10-09. Jump up ^ "The OpenZFS project launches". LWN.net. 2013-09-17. Retrieved 2013-10-01. Jump up ^ "OpenZFS – Communities co-operating on ZFS code and features". freebsdnews.net. 2013-09-23. Retrieved 2014-03-14. Jump up ^ The Extended file system (Ext) has metadata structure copied from UFS. "Rémy Card (Interview, April 1998)". April Association. April 19, 1999. Retrieved 2012-02-08. (In French) Jump up ^ Vijayan Prabhakaran (2006). "IRON FILE SYSTEMS" (PDF). Doctor of Philosophy in Computer Sciences. University of Wisconsin-Madison. Retrieved 9 June 2012. Jump up ^ "Parity Lost and Parity Regained". Jump up ^ "An Analysis of Data Corruption in the Storage Stack" (PDF). Jump up ^ "Impact of Disk Corruption on Open-Source DBMS" (PDF). Jump up ^ Kadav, Asim; Rajimwale, Abhishek. "Reliability Analysis of ZFS" (PDF). Jump up ^ Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. "End-to-end Data Integrity for File Systems: A ZFS Case Study" (PDF). Madison: Computer Sciences Department, University of Wisconsin. p. 14. Retrieved December 6, 2010. Jump up ^ Larabel, Michael. "Benchmarking ZFS and UFS On FreeBSD vs. EXT4 & Btrfs On Linux". Phoronix Media 2012. Retrieved 21 November 2012. Jump up ^ Larabel, Michael. "Can DragonFlyBSD's HAMMER Compete With Btrfs, ZFS?". Phoronix Media 2012. Retrieved 21 November 2012. ^ Jump up to: a b c Bonwick, Jeff (2005-12-08). "ZFS End-to-End Data Integrity". blogs.oracle.com. Retrieved 2013-09-19. Jump up ^ Cook, Tim (November 16, 2009). "Demonstrating ZFS Self-Healing". blogs.oracle.com. Retrieved 2015-02-01. Jump up ^ Ranch, Richard (2007-05-04). "ZFS, copies, and data protection". blogs.oracle.com. Retrieved 2015-02-02. Jump up ^ "Difference between Desktop edition and RAID (Enterprise) edition drives". ^ Jump up to: a b c d Bonwick, Jeff (2005-11-17). "RAID-Z". Jeff Bonwick's Blog. Oracle Blogs. Retrieved 2015-02-01. Jump up ^ "Why RAID 6 stops working in 2019". ZDNet. February 22, 2010. Retrieved October 26, 2014. Jump up ^ "Actually it's a n-way mirror". c0t0d0s0.org. 2013-09-04. Retrieved 2013-11-19. Jump up ^ "No fsck utility equivalent exists for ZFS. This utility has traditionally served two purposes, those of file system repair and file system validation." "Checking ZFS File System Integrity". Oracle. Retrieved 25 November 2012. Jump up ^ "If you have consumer-quality drives, consider a weekly scrubbing schedule. If you have datacenter-quality drives, consider a monthly scrubbing schedule." "ZFS Scrubs". freenas.org. Retrieved 25 November 2012.[dead link] Jump up ^ "You should also run a scrub prior to replacing devices or temporarily reducing a pool's redundancy to ensure that all devices are currently operational." "ZFS Best Practices Guide". solarisinternals.com. Retrieved 25 November 2012.[dead link] Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS Best Practices Guide". Solaris Performance Wiki. Retrieved October 2, 2007.[dead link] Jump up ^ Leventhal, Adam. "Bug ID: 6854612 triple-parity RAID-Z". Sun Microsystems. Retrieved July 17, 2009.[dead link] Jump up ^ Leventhal, Adam (July 16, 2009). "6854612 triple-parity RAID-Z". zfs-discuss (Mailing list). Retrieved July 17, 2009.[dead link] Jump up ^ "WHEN TO (AND NOT TO) USE RAID-Z". Oracle. Retrieved 13 May 2013. Jump up ^ "Solaris ZFS Enables Hybrid Storage Pools—Shatters Economic and Performance Barriers" (PDF). Sun.com. September 7, 2010. Retrieved November 4, 2011. Jump up ^ "Brendan's blog » ZFS L2ARC". Dtrace.org. Retrieved 2012-10-05. Jump up ^ "Solaris ZFS Performance Tuning: Synchronous Writes and the ZIL". Constantin.glez.de. 2010-07-20. Retrieved 2012-10-05. Jump up ^ Jeff Bonwick. "128-bit storage: are you high?". oracle.com. Retrieved May 29, 2015. ^ Jump up to: a b Bonwick, Jeff (October 31, 2005). "ZFS: The Last Word in Filesystems". blogs.oracle.com. Retrieved June 22, 2013. Jump up ^ "Solaris ZFS Administration Guide". Oracle Corporation. Retrieved February 11, 2011. Jump up ^ "ZFS On-Disk Specification" (PDF). Sun Microsystems, Inc. 2006.[dead link] See section 2.4. Jump up ^ Eric Sproul (2009-05-21). "ZFS Nuts and Bolts". slideshare.net. pp. 30–31. Retrieved 2014-06-08. Jump up ^ "Unix.com". Unix.com. November 13, 2007. Retrieved November 4, 2011. Jump up ^ "ZFS Deduplication". blogs.oracle.com. Jump up ^ Gary Sims (4 January 2012). "Building ZFS Based Network Attached Storage Using FreeNAS 8" (Blog). TrainSignal Training. TrainSignal, Inc. Retrieved 9 June 2012. Jump up ^ Ray Van Dolson (May 2011). "[zfs-discuss] Summary: Deduplication Memory Requirements". zfs-discuss mailing list. Archived from the original on 2012-04-25. Jump up ^ "ZFSTuningGuide". Jump up ^ Chris Mellor (October 12, 2012). "GreenBytes brandishes full-fat clone VDI pumper". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (June 1, 2012). "Newcomer gets out its box, plans to sell it cheaply to all comers". The Register. Retrieved August 29, 2013. Jump up ^ Chris Mellor (2014-12-11). "Dedupe, dedupe... dedupe, dedupe, dedupe: Oracle polishes ZFS diamond". The Register. Retrieved 2014-12-17. Jump up ^ "Encrypting ZFS File Systems". Jump up ^ "Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)". Jump up ^ "Solaris ZFS Administration Guide". Chapter 6 Managing ZFS File Systems. Retrieved March 17, 2009.[dead link] ^ Jump up to: a b "Smokin' Mirrors". blogs.oracle.com. May 2, 2006. Retrieved February 13, 2012. Jump up ^ "ZFS Block Allocation". Jeff Bonwick's Weblog. November 4, 2006. Retrieved February 23, 2007. Jump up ^ "Ditto Blocks — The Amazing Tape Repellent". Flippin' off bits Weblog. May 12, 2006. Retrieved March 1, 2007. Jump up ^ "Adding new disks and ditto block behaviour". Retrieved October 19, 2009.[dead link] Jump up ^ "OpenSolaris.org". Sun Microsystems. Retrieved May 22, 2009.[dead link] Jump up ^ "Bug ID 4852783: reduce pool capacity". OpenSolaris Project. Retrieved March 28, 2009.[dead link] Jump up ^ Goebbels, Mario (April 19, 2007). "Permanently removing vdevs from a pool". zfs-discuss (Mailing list).[dead link] Jump up ^ "Expand-O-Matic RAID-Z". Adam Leventhal. April 7, 2008. Jump up ^ "zpool(1M)". Download.oracle.com. June 11, 2010. Retrieved November 4, 2011. Jump up ^ Leventhal, Adam. "Triple-Parity RAID-Z". Adam Leventhal's blog. Retrieved 19 December 2013. Jump up ^ brendan (December 2, 2008). "A quarter million NFS IOPS". Oracle Sun. Retrieved January 28, 2012. Jump up ^ "Oracle Has Killed OpenSolaris". Techie Buzz. August 14, 2010. Retrieved July 17, 2013. Jump up ^ "Upgrading from OpenSolaris". Retrieved September 24, 2011. Jump up ^ "OpenZFS on OS X". openzfsonosx.org. 2014-09-29. Retrieved 2014-11-23. ^ Jump up to: a b "Features – OpenZFS – Feature flags". OpenZFS. Retrieved 22 September 2013. Jump up ^ "MacZFS: Official Site for the Free ZFS for Mac OS". code.google.com. MacZFS. Retrieved 2014-03-02. Jump up ^ "ZEVO Wiki Site/ZFS Pool And Filesystem Versions". GreenBytes, Inc. 2012-09-15. Retrieved 22 September 2013. Jump up ^ "Github zfs-port branch". Jump up ^ "NetBSD Google Summer of Code projects: ZFS". Jump up ^ Dawidek, Paweł (April 6, 2007). "ZFS committed to the FreeBSD base". Retrieved April 6, 2007. Jump up ^ "Revision 192498". May 20, 2009. Retrieved May 22, 2009. Jump up ^ "ZFS v13 in 7-STABLE". May 21, 2009. Retrieved May 22, 2009.[dead link] Jump up ^ "iSCSI target for FreeBSD". Retrieved August 6, 2011. Jump up ^ "FreeBSD 8.0-RELEASE Release Notes". FreeBSD. Retrieved November 27, 2009. Jump up ^ "FreeBSD 8.0-STABLE Subversion logs". FreeBSD. Retrieved February 5, 2010. Jump up ^ "FreeBSD 8.2-RELEASE Release Notes". FreeBSD. Retrieved March 9, 2011. Jump up ^ "HEADS UP: ZFS v28 merged to 8-STABLE". June 6, 2011. Retrieved June 11, 2011. Jump up ^ "FreeBSD 8.3-RELEASE Announcement". Retrieved June 11, 2012. Jump up ^ Pawel Jakub Dawidek. "ZFS v28 is ready for wider testing.". Retrieved August 31, 2010. Jump up ^ "FreeBSD 9.0-RELEASE Release Notes". FreeBSD. Retrieved January 12, 2012. Jump up ^ "FreeBSD 9.2-RELEASE Release Notes". FreeBSD. Retrieved September 30, 2013. Jump up ^ "NAS4Free: Features". Retrieved 13 January 2015. Jump up ^ "Debian GNU/kFreeBSD FAQ". Is there ZFS support?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". Can I use ZFS as root or /boot file system?. Retrieved 2013-09-24. Jump up ^ "Debian GNU/kFreeBSD FAQ". What grub commands are necessary to boot Debian/kFreeBSD from a zfs root?. Retrieved 2013-09-24. Jump up ^ Larabel, Michael (2010-09-10). "Debian GNU/kFreeBSD Becomes More Interesting". Phoronix. Retrieved 2013-09-24. Jump up ^ Aditya Rajgarhia and Ashish Gehani (November 23, 2012). "Performance and Extension of User Space File Systems" (PDF). Jump up ^ "Linus on GPLv3 and ZFS". Lwn.net. June 12, 2007. Retrieved November 4, 2011. Jump up ^ Jeremy Andrews (April 19, 2007). "Linux: ZFS, Licenses and Patents". Archived from the original on 12 June 2011. Retrieved April 21, 2007. Jump up ^ Behlendorf, Brian (2013-05-28). "spl/zfs-0.6.1 released". zfs-announce mailing list. Retrieved 2013-10-09. Jump up ^ "ZFS on Linux". Retrieved 29 August 2013. ^ Jump up to: a b Matt Ahrens; Brian Behlendorf (2013-09-17). "LinuxCon 2013: OpenZFS" (PDF). linuxfoundation.org. Retrieved 2013-11-13. Jump up ^ "ZFS – Ubuntu documentation". ubuntu.com. Retrieved 2013-10-09. Jump up ^ "ZFS – Gentoo documentation". gentoo.org. Retrieved 2013-10-09. Jump up ^ "ZFS root". Slackware ZFS root. SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS root (builtin)". Slackware ZFS root (builtin). SlackWiki.com. Retrieved 2014-08-13. Jump up ^ "ZFS on Linux". zfsonlinux.org. Retrieved 2014-08-13. Jump up ^ Ricardo Correia (2008-09-13). "ZFS on FUSE/Linux". Retrieved 2013-11-13. Jump up ^ Darshin (August 24, 2010). "ZFS Port to Linux (all versions)". Retrieved August 31, 2010.[dead link] Jump up ^ "Where can I get the ZFS for Linux source code?". Archived from the original on 8 October 2011. Retrieved 29 August 2013. Jump up ^ Phoronix (November 22, 2010). "Running The Native ZFS Linux Kernel Module, Plus Benchmarks". Retrieved December 7, 2010. ^ Jump up to: a b "KQ ZFS Linux Is No Longer Actively Being Worked On". June 10, 2011. Jump up ^ "zfs-linux / zfs". Jump up ^ Brown, David. "A Conversation with Jeff Bonwick and Bill Moore". ACM Queue. Association for Computing Machinery. Retrieved 17 November 2015. Jump up ^ "ZFS: the last word in file systems". Sun Microsystems. September 14, 2004. Archived from the original on April 28, 2006. Retrieved April 30, 2006. Jump up ^ Matthew Ahrens (November 1, 2011). "ZFS 10 year anniversary". Retrieved July 24, 2012. Jump up ^ "Sun Celebrates Successful One-Year Anniversary of OpenSolaris". Sun Microsystems. June 20, 2006. Jump up ^ "ZFS FAQ at OpenSolaris.org". Sun Microsystems. Retrieved May 18, 2011. The largest SI prefix we liked was 'zetta' ('yotta' was out of the question)[dead link] Jump up ^ Jeff Bonwick (May 3, 2006). "You say zeta, I say zetta". Jeff Bonwick's Blog. Retrieved April 23, 2012. Jump up ^ "Oracle and NetApp dismiss ZFS lawsuits". theregister.co.uk. 2010-09-09. Retrieved 2013-12-24. Jump up ^ "OpenZFS History". OpenZFS. Retrieved 2013-09-24. Jump up ^ "illumos FAQs". illumos. Retrieved 2013-09-24. Jump up ^ "Sun rolls out its own storage appliances". techworld.com.au. 2008-11-11. Retrieved 2013-11-13. Jump up ^ Chris Mellor (2013-10-02). "Oracle muscles way into seat atop the benchmark with hefty ZFS filer". theregister.co.uk. Retrieved 2014-07-07. Jump up ^ "Unified ZFS Storage Appliance built in Silicon Valley by iXsystem". ixsystems.com. Retrieved 2014-07-07. Jump up ^ "ReadyDATA 516 - Unified Network Storage" (PDF). netgear.com. Retrieved 2014-07-07. Jump up ^ "Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions". Oracle Corporation. 2010. Retrieved February 11, 2011. Jump up ^ "Oracle Solaris ZFS Version Descriptions". Oracle Corporation. Retrieved 2013-09-23. Jump up ^ Siden, Christopher (January 2012). "ZFS Feature Flags" (PDF). Illumos Meetup. Delphix. p. 4. Retrieved 2013-09-22. Jump up ^ "/usr/src/uts/common/sys/fs/zfs.h (line 338)". illumos (GitHub). Retrieved 2013-11-16. Jump up ^ "/usr/src/uts/common/fs/zfs/zfeature.c (line 89)". illumos (GitHub). Retrieved 2013-11-16. ^ Jump up to: a b c "While under Sun Microsystems' control, there were bi-weekly snapshots of Solaris Nevada (the codename for the next-generation Solaris OS to eventually succeed Solaris 10) and this new code was then pulled into new OpenSolaris preview snapshots available at Genunix.org. The stable releases of OpenSolaris are based off of these Nevada builds." Larabel, Michael. "It Looks Like Oracle Will Stand Behind OpenSolaris". Phoronix Media. Retrieved 21 November 2012. Jump up ^ Ljubuncic, Igor (23 May 2011). "OpenIndiana — there's still hope". DistroWatch. Jump up ^ "Welcome to Project OpenIndiana!". Project OpenIndiana. 10 September 2010. Retrieved 14 September 2010. Jump up ^ "Porting ZFS to OSX". zfs-discuss. April 27, 2006. Retrieved April 30, 2006.[dead link] Jump up ^ "Apple: Leopard offers limited ZFS read-only". MacNN. June 12, 2007. Retrieved June 23, 2007. Jump up ^ "Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard". Ars Technica. October 7, 2007. Retrieved October 7, 2007. Jump up ^ Ché Kristo (November 18, 2007). "ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) " ideas are free". Retrieved December 30, 2007.[dead link] Jump up ^ ZFS.macosforge.org[dead link] Jump up ^ http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com Jump up ^ "Snow Leopard (archive.org cache)". July 21, 2008. Archived from the original on 2008-07-21. Jump up ^ "Snow Leopard". June 9, 2009. Retrieved June 10, 2008. Jump up ^ "maczfs – Official Site for the Free ZFS for Mac OS – Google Project Hosting". Google. Retrieved July 30, 2012. Jump up ^ "zfs-macos | Google Groups". Google. Retrieved November 4, 2011. Jump up ^ "Distribution – OpenZFS". OpenZFS. Retrieved 17 September 2013. Bibliography[edit] Watanabe, Scott (November 23, 2009). "Solaris ZFS Essentials" (1st ed.). Prentice Hall. p. 256. ISBN 0-13-700010-3.[dead link] External links[edit] The OpenZFS Project Comparison of SVM mirroring and ZFS mirroring EON ZFS Storage (NAS) distribution ZFS on Linux Homepage End-to-end Data Integrity for File Systems: A ZFS Case Study ZFS – The Zettabyte File System (archived link, February 28, 2013) ZFS and RAID-Z: The Über-FS? ZFS: The Last Word In File Systems, by Jeff Bonwick and Bill Moore Visualizing the ZFS intent log (ZIL), April 2013, by Aaron Toponce [show] v t e Solaris [show] v t e The FreeBSD Project [show] v t e OS X [show] v t e File systems Categories: 2005 softwareCompression file systemsDisk file systemsLinux kernel-supported file systemsRAIDSoftware using the CDDL licenseSun Microsystems software Navigation menu Create accountNot logged inTalkContributionsLog inArticleTalkReadEditView history ontents [hide] 1 Features 1.1 Data integrity 1.1.1 ZFS data integrity 1.2 RAID 1.2.1 ZFS and hardware RAID 1.2.2 Software RAID using ZFS 1.2.3 Resilvering and scrub 1.3 Storage pools 1.4 ZFS cache: ARC (L1), L2ARC, ZIL 1.5 Capacity 1.6 Copy-on-write transactional model 1.7 Snapshots and clones 1.8 Sending and receiving snapshots 1.9 Dynamic striping 1.10 Variable block sizes 1.11 Lightweight filesystem creation 1.12 Cache management 1.13 Adaptive endianness 1.14 Deduplication 1.15 Encryption 1.16 Additional capabilities 2 Limitations 3 Platforms 3.1 Solaris 3.1.1 Solaris 10 update 2 and later 3.1.2 Solaris 11 3.1.3 OpenSolaris 3.1.4 OpenIndiana 3.2 BSD 3.2.1 OS X 3.2.2 DragonFlyBSD 3.2.3 NetBSD 3.2.4 FreeBSD 3.2.5 MidnightBSD 3.2.6 PC-BSD 3.2.7 FreeNAS 3.2.8 ZFS Guru 3.2.9 NAS4Free 3.2.10 Debian GNU/kFreeBSD 3.3 Linux 3.3.1 Native ZFS on Linux 3.3.2 Linux FUSE 3.3.3 KQ InfoTech 3.4 List of operating systems supporting ZFS 4 History 4.1 Open source implementations 4.2 Use in commercial products 4.3 Detailed release history 4.4 OS X 5 See also 6 References 7 Bibliography 8 External links Features[edit] Data integrity[edit] See also: Hard disk error rates and handling and Silent data corruption One major feature that distinguishes ZFS from other file systems is that ZFS is designed with a focus on data integrity. That is, it is designed to protect the user's data on disk against silent data corruption caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.Data integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems—​such as UFS, Ext,[7] XFS, JFS, or NTFS—​nor hardware RAID provide sufficient protection against such problems (hardware RAID has some issues with data integrity).[8][9][10][11] Initial research indicates that ZFS protects data better than earlier efforts.[12][13] While it is also faster than UFS,[14][15] it can be seen as a replacement for UFS. ZFS data integrity[edit] For ZFS, data integrity is achieved by using a (Fletcher-based) checksum or a (SHA-256) hash throughout the file system tree.[16] Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree.[16] In-flight data corruption or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.[16] When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it "should" be. If the checksums match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching checksums.[17] If the storage pool consists of a single disk, it is possible to provide such redundancy by specifying copies=2 (or copies=3), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for copies=3, reducing to one third) the storage capacity of the disk.[18] If redundancy exists, ZFS will fetch a copy of the data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update the faulty copy with known-good data so that redundancy can be restored. RAID[edit] ZFS and hardware RAID[edit] If the disks are connected to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID functionality). If a hardware RAID card is used, ZFS always detects all data corruption but cannot always repair data corruption because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS to be able to guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.  There are several reasons as to why it is better to rely solely on ZFS by using several independent disks and RAID-Z or mirroring. When using hardware RAID, the controller usually adds controller-dependent data to the drives which prevents software RAID from accessing the user data. While it is possible to read the data with a compatible hardware RAID controller, this inconveniences consumers as a compatible controller usually isn't readily available. Using the JBOD/RAID-Z combination, any disk controller can be used to resume operation after a controller failure. Note that hardware RAID configured as JBOD may still detach drives that do not respond in time (as has been seen with many energy-efficient consumer-grade hard drives), and as such, may require TLER/CCTL/ERC-enabled drives to prevent drive dropouts.[19] Software RAID using ZFS[edit] ZFS offers software RAID through its RAID-Z and mirroring organization schemes.  RAID-Z is a data/parity distribution scheme like RAID-5, but uses dynamic stripe width: every block is its own RAID stripe, regardless of blocksize, resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 because it does not need to perform the usual read-modify-write sequence.[20]  As all stripes are of different sizes, RAID-Z reconstruction has to traverse the filesystem metadata to determine the actual RAID-Z geometry. This would be impossible if the filesystem and the RAID array were separate products, whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data. Going through the metadata means that ZFS can validate every block against its 256-bit checksum as it goes, whereas traditional RAID products usually cannot do this.[20] In addition to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, and if the data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.[20] RAID-Z does not require any special hardware: it does not need NVRAM for reliability, and it does not need write buffering for good performance. With RAID-Z, ZFS provides fast, reliable storage using cheap, commodity disks.[20] There are three different RAID-Z modes: RAID-Z1 (similar to RAID 5, allows one disk to fail), RAID-Z2 (similar to RAID 6, allows two disks to fail), and RAID-Z3 (allows three disks to fail). The need for RAID-Z3 arose recently because RAID configurations with future disks (say, 6–10 TB) may take a long time to repair, the worst case being weeks. During those weeks, the rest of the disks in the RAID are stressed more because of the additional intensive repair process and might subsequently fail, too. By using RAID-Z3, the risk involved with disk replacement is reduced.[21] Mirroring, the other ZFS RAID option, is essentially the same as RAID 1, allowing any number of disks to be mirrored.[22] fsck must be run on an offline filesystem, which means the filesystem must be unmounted and is not usable while being repaired. scrub does not need the ZFS filesystem to be taken offline; scrub is designed to be used on a mounted, live filesystem. fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt. scrub checks everything, including metadata and the data. The effect can be observed by comparing fsck to scrub times – sometimes a fsck on a large RAID completes in a few minutes, which means only the metadata was checked. Traversing all metadata and data on a large RAID takes many hours, which is exactly what scrub does. The official recommendation from Sun/Oracle is to scrub enterprise-level disks once a month, and cheaper commodity disks once a week.[24][25] Storage pools[edit] Unlike traditional file systems which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called zpools. A zpool is constructed of virtual devices (vdevs), which are themselves constructed of block devices: files, hard drive partitions, or entire drives, with the latter being the recommended usage.[26] Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to RAID 0), as a mirror (RAID 1) of two or more devices, as a RAID-Z group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.[27] In July 2009, triple-parity RAID-Z3 was added to OpenSolaris.[28][29] RAID-Z is a data-protection technology featured by ZFS in order to reduce the block overhead in mirroring.[30] Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy, because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same base capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700 GB drive, will have a capacity of 4×500 GB. In addition, pools can have hot spares to compensate for failing disks. When mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.  Storage pool composition is not limited to similar devices, but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed. Arbitrary storage device types can be added to existing pools to expand their size at any time.[31] The storage capacity of all vdevs is available to all of the file system instances in the zpool. A quota can be set to limit the amount of space a file system instance can occupy, and a reservation can be set to guarantee that space will be available to a file system instance. ZFS cache: ARC (L1), L2ARC, ZIL[edit] ZFS uses different layers of disk cache to speed up read and write operations. Ideally, all data should be stored in RAM, but that is too expensive. Therefore, data is automatically cached in a hierarchy to optimize performance vs cost.[32] Frequently accessed data is stored in RAM, and less frequently accessed data can be stored on slower media, such as SSD disks. Data that is not often accessed is not cached and left on the slow hard drives. If old data is suddenly read a lot, ZFS will automatically move it to SSD disks or to RAM. The first level of disk cache is RAM, which uses a variant of the ARC algorithm. It is similar to a level 1 CPU cache. RAM will always be used for caching, thus this level is always present. There are claims that ZFS servers must have huge amounts of RAM, but that is not true. It is a misinterpretation of the desire to have large ARC disk caches. The ARC is very clever and efficient, which means disks will often not be touched at all, provided the ARC size is sufficiently large. In the worst case, if the RAM size is very small (say, 1 GB), there will hardly be any ARC at all; in this case, ZFS always needs to reach for the disks. This means read performance degrades to disk speed. The second level of disk cache are SSD disks. This level is optional, and is easy to add or remove during live usage, as there is no need to shut down the zpool. There are two different caches; one cache for reads, and one for writes. The read SSD cache is called L2ARC and is similar to a level 2 CPU cache. The L2ARC will also considerably speed up Deduplication if the entire Dedup table can be cached in L2ARC. It can take several hours to fully populate the L2ARC (before it has decided which data are "hot" and should be cached). If the L2ARC device is lost, all reads will go out to the disks which slows down performance, but nothing else will happen (no data will be lost). The write SSD cache is called the Log Device, and it is used by the ZIL (ZFS intent log). ZIL basically turns synchronous writes into asynchronous writes, which helps e.g. NFS or databases.[33] All data is written to the ZIL like a journal log, but only read after a crash. Thus, the ZIL data is normally never read. In case there is no separate log device added to the zpool, a part of the zpool will automatically be used as ZIL, thus there is always a ZIL on every zpool. It is important that the log device use a disk with low latency. For improved performance, a disk consisting of battery-backed RAM should be used. Because the log device is written to often, an SSD disk will eventually be worn out, but a RAM disk will not. If the log device is lost, it is possible to lose the latest writes, therefore the log device should be mirrored. In earlier versions of ZFS, loss of the log device could result in loss of the entire zpool, therefore one should upgrade ZFS if planning to use a separate log device. Capacity[edit] ZFS is a 128-bit file system,[34][35] so it can address 1.84 × 1019 times more data than 64-bit systems such as Btrfs. The limitations of ZFS are designed to be so large that they should not be encountered in the foreseeable future. 248: number of entries in any individual directory[36] 16 exbibytes (264 bytes): maximum size of a single file 16 exbibytes: maximum size of any attribute 256 zebibytes (278 bytes): maximum size of any zpool 256: number of attributes of a file (actually constrained to 248 for the number of files in a directory) 264: number of devices in any zpool 264: number of zpools in a system 264: number of file systems in a zpool Copy-on-write transactional model[edit] ZFS uses a copy-on-write transactional object model. All block pointers within the filesystem contain a 256-bit checksum or 256-bit hash (currently a choice between Fletcher-2, Fletcher-4, or SHA-256)[37] of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their checksums (see Merkle signature scheme).  Snapshots and clones[edit] An advantage of copy-on-write is that, when ZFS writes new data, the blocks containing the old data can be retained, allowing a snapshot version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored. They are also space efficient, since any unchanged data is shared among the file system and its snapshots.  Writeable snapshots ("clones") can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the Copy-on-write principle. Sending and receiving snapshots[edit] ZFS file systems can be moved to other pools, also on remote hosts over the network, as the send command creates a stream representation of the file system's state. This stream can either describe complete contents of the file system at a given snapshot, or it can be a delta between snapshots. Computing the delta stream is very efficient, and its size depends on the number of blocks changed between the snapshots. This provides an efficient strategy, e.g. for synchronizing offsite backups or high availability mirrors of a pool. Dynamic striping[edit] Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus, all disks in a pool are used, which balances the write load across them. Variable block sizes[edit] ZFS uses variable-sized blocks, with 128 KB as the default size. Available features allow the administrator to tune the maximum block size which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).[38]");
  });
  
  router.get('/perfendpoint', function(req, res) {
    db.todos.find(function(err, data) {
      res.end("thebearhasspoken");
    });
  });

  router.post('/api/todos', function(req, res) {
    db.todos.insert(req.body, function(err, data) {
      res.json(data);
    });

  });

  router.put('/api/todos', function(req, res) {

    db.todos.update({
      _id: mongojs.ObjectId(req.body._id)
    }, {
      isCompleted: req.body.isCompleted,
      todo: req.body.todo
    }, {}, function(err, data) {
      res.json(data);
    });

  });

  router.delete('/api/todos/:_id', function(req, res) {
    db.todos.remove({
      _id: mongojs.ObjectId(req.params._id)
    }, '', function(err, data) {
      res.json(data);
    });

  });

  module.exports = router;

}());
